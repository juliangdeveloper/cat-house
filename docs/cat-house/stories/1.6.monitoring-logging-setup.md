# Story 1.6: Monitoring and Logging Setup

**Status:** Draft  
**Epic:** EPIC-1 - Core Platform Infrastructure  
**Created:** November 30, 2025

---

## Story

**As a** platform architect,  
**I want** to implement comprehensive monitoring, metrics, and alerting for all services,  
**so that** we can proactively detect issues, track performance trends, and maintain system reliability.

**Note:** Structured logging with CloudWatch is already configured in Story 1.5. This story builds upon that foundation to add metrics collection, dashboards, and alerting.

---

## Acceptance Criteria

**Prerequisites:** Story 1.5 completed (structured JSON logging with correlation IDs and CloudWatch integration)

1. **Application metrics collection**
   - Prometheus metrics endpoint (`/metrics`) in all 5 services
   - HTTP request metrics (count, latency, status codes)
   - Database connection pool metrics
   - Custom business metrics (user actions, installations)
   - Metrics scraped and stored in CloudWatch

2. **CloudWatch dashboards created**
   - Service health overview dashboard
   - API performance dashboard (latency, throughput, errors)
   - Database metrics dashboard
   - Business metrics dashboard
   - All dashboards accessible and documented

3. **CloudWatch alarms configured**
   - Critical: Service down, high error rate (5XX > 5%)
   - Warning: High latency (p95 > 500ms), elevated CPU/memory
   - Info: Unusual traffic patterns
   - All alarms tested and verified

4. **Alert notification system**
   - SNS topics for critical, warning, and info alerts
   - Email notifications configured
   - Slack webhook integration (optional)
   - Runbooks linked to each alarm
   - On-call rotation documented

5. **Log queries and insights**
   - Common CloudWatch Logs Insights queries saved
   - Error tracking and aggregation queries
   - Performance analysis queries
   - Request tracing by correlation ID
   - Query documentation for team

6. **Distributed tracing (Optional - Phase 2)**
   - OpenTelemetry instrumentation (deferred)
   - AWS X-Ray integration (deferred)
   - Cross-service trace visualization (deferred)

---

## Tasks / Subtasks

- [ ] Implement Prometheus metrics endpoints (AC: 1)
  - [ ] Add prometheus_client to all 5 services
  - [ ] Create metrics middleware for HTTP requests
  - [ ] Add database connection pool metrics
  - [ ] Expose /metrics endpoint
  - [ ] Test metrics collection locally

- [ ] Configure CloudWatch metrics export (AC: 1)
  - [ ] Set up CloudWatch metrics namespace
  - [ ] Configure metric filters from logs
  - [ ] Create custom metrics for business events
  - [ ] Verify metrics appear in CloudWatch

- [ ] Create CloudWatch dashboards (AC: 2)
  - [ ] Service health dashboard (all 5 services)
  - [ ] API performance dashboard
  - [ ] Database metrics dashboard
  - [ ] Business metrics dashboard
  - [ ] Export and version control dashboard JSON

- [ ] Set up CloudWatch alarms (AC: 3)
  - [ ] Define alarm thresholds per metric
  - [ ] Create critical alarms (service down, high errors)
  - [ ] Create warning alarms (latency, resource usage)
  - [ ] Test alarm triggering
  - [ ] Verify alarm state transitions

- [ ] Configure alert notifications (AC: 4)
  - [ ] Create SNS topics (critical, warning, info)
  - [ ] Set up email subscriptions
  - [ ] Configure Slack webhook (optional)
  - [ ] Test notification delivery
  - [ ] Document escalation procedures

- [ ] Create alarm runbooks (AC: 4)
  - [ ] Write runbook for each alarm type
  - [ ] Document troubleshooting steps
  - [ ] Link runbooks to alarms
  - [ ] Test runbook procedures

- [ ] Set up CloudWatch Logs Insights queries (AC: 5)
  - [ ] Create saved queries for common patterns
  - [ ] Error aggregation and tracking queries
  - [ ] Performance analysis queries
  - [ ] Request tracing queries
  - [ ] Document query library for team

- [ ] (Optional) Plan distributed tracing (AC: 6)
  - [ ] Research OpenTelemetry integration
  - [ ] Document X-Ray setup plan
  - [ ] Defer implementation to Phase 2

---

## Dev Notes

### Prerequisites from Story 1.5

✅ **Already Implemented:**
- Structured JSON logging with Loguru
- Correlation IDs (X-Trace-ID) via middleware
- CloudWatch log groups for all 5 services
- Log retention policies (30 days staging, 90 days prod)
- Request/response logging

### Current Services

```
cat-house-backend/
├── auth-service/         (Port 8005)
├── catalog-service/      (Port 8002)
├── installation-service/ (Port 8003)
├── proxy-service/        (Port 8004)
└── health-aggregator/    (Port 8000)
```

### Metrics Implementation

### Metrics Implementation

**Install Dependencies:**

```bash
# Add to cat-house-backend/{service}/requirements.txt
prometheus-client==0.19.0
```

**Create Metrics Module:**

```python
# cat-house-backend/{service}/app/metrics.py
from prometheus_client import Counter, Histogram, Gauge, CollectorRegistry, generate_latest
from prometheus_client.core import REGISTRY
import time
from app.config import settings

# HTTP Request metrics
http_requests_total = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status', 'service'],
    registry=REGISTRY
)

http_request_duration_seconds = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration in seconds',
    ['method', 'endpoint', 'service'],
    buckets=[0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0],
    registry=REGISTRY
)

http_requests_in_progress = Gauge(
    'http_requests_in_progress',
    'HTTP requests currently being processed',
    ['method', 'service'],
    registry=REGISTRY
)

# Database metrics
db_connections_active = Gauge(
    'db_connections_active',
    'Number of active database connections',
    ['service'],
    registry=REGISTRY
)

db_query_duration_seconds = Histogram(
    'db_query_duration_seconds',
    'Database query duration in seconds',
    ['service', 'operation'],
    buckets=[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],
    registry=REGISTRY
)

# Business metrics
business_events_total = Counter(
    'business_events_total',
    'Total business events',
    ['service', 'event_type'],
    registry=REGISTRY
)

# Service health
service_health_status = Gauge(
    'service_health_status',
    'Service health status (1=healthy, 0=unhealthy)',
    ['service'],
    registry=REGISTRY
)

def track_request_metrics(method: str, endpoint: str, status_code: int, duration: float):
    """Track HTTP request metrics."""
    http_requests_total.labels(
        method=method,
        endpoint=endpoint,
        status=status_code,
        service=settings.SERVICE_NAME
    ).inc()
    
    http_request_duration_seconds.labels(
        method=method,
        endpoint=endpoint,
        service=settings.SERVICE_NAME
    ).observe(duration)

def track_business_event(event_type: str):
    """Track business events (user registration, cat installation, etc.)."""
    business_events_total.labels(
        service=settings.SERVICE_NAME,
        event_type=event_type
    ).inc()
```

**Add Metrics Middleware:**

```python
# cat-house-backend/{service}/app/middleware.py (add to existing)
import time
from app.metrics import (
    track_request_metrics,
    http_requests_in_progress,
    service_health_status
)
from app.config import settings

async def metrics_middleware(request: Request, call_next):
    """Track metrics for all requests."""
    # Track in-progress requests
    http_requests_in_progress.labels(
        method=request.method,
        service=settings.SERVICE_NAME
    ).inc()
    
    start_time = time.time()
    
    try:
        response = await call_next(request)
        duration = time.time() - start_time
        
        # Track request metrics
        track_request_metrics(
            method=request.method,
            endpoint=request.url.path,
            status_code=response.status_code,
            duration=duration
        )
        
        return response
    
    finally:
        # Decrement in-progress counter
        http_requests_in_progress.labels(
            method=request.method,
            service=settings.SERVICE_NAME
        ).dec()
```

**Add Metrics Endpoint:**

```python
# cat-house-backend/{service}/app/routers/metrics.py
from fastapi import APIRouter, Response
from prometheus_client import generate_latest, CONTENT_TYPE_LATEST

router = APIRouter(tags=["metrics"])

@router.get("/metrics")
async def metrics():
    """Prometheus metrics endpoint."""
    return Response(
        content=generate_latest(),
        media_type=CONTENT_TYPE_LATEST
    )
```

**Register Metrics Router:**

```python
# cat-house-backend/{service}/app/main.py (update)
from app.routers import metrics
from app.middleware import metrics_middleware, correlation_id_middleware

app = FastAPI(title="Auth Service")

# Add middlewares (order matters - metrics should be outermost)
app.middleware("http")(metrics_middleware)
app.middleware("http")(correlation_id_middleware)

# Include metrics router
app.include_router(metrics.router)

# Set initial health status
from app.metrics import service_health_status
from app.config import settings

@app.on_event("startup")
async def startup():
    service_health_status.labels(service=settings.SERVICE_NAME).set(1)

@app.on_event("shutdown")
async def shutdown():
    service_health_status.labels(service=settings.SERVICE_NAME).set(0)
```

**Track Business Events Example:**

```python
# In your route handlers
from app.metrics import track_business_event

@router.post("/users")
async def create_user(user: UserCreate, db: Session = Depends(get_db)):
    # ... create user logic ...
    
    # Track business event
    track_business_event("user_created")
    
    return {"id": user.id}

@router.post("/installations")
async def create_installation(installation: InstallationCreate, db: Session = Depends(get_db)):
    # ... create installation logic ...
    
    # Track business event
    track_business_event("installation_created")
    
    return {"id": installation.id}
```

### CloudWatch Metrics Export

**Create Metric Filters from Logs:**

```bash
# Extract error count from logs
aws logs put-metric-filter \
  --log-group-name /ecs/cat-house/staging/auth-service \
  --filter-name ErrorCount \
  --filter-pattern '[timestamp, level=ERROR, ...]' \
  --metric-transformations \
    metricName=ErrorCount,\
    metricNamespace=CatHouse/Services,\
    metricValue=1,\
    defaultValue=0

# Extract latency from logs
aws logs put-metric-filter \
  --log-group-name /ecs/cat-house/staging/auth-service \
  --filter-name RequestLatency \
  --filter-pattern '[timestamp, level, service, trace_id, message, ..., duration_ms]' \
  --metric-transformations \
    metricName=RequestLatency,\
    metricNamespace=CatHouse/Services,\
    metricValue=$duration_ms,\
    unit=Milliseconds
```

**Or use EMF (Embedded Metric Format) in logs:**

```python
# cat-house-backend/{service}/app/cloudwatch_metrics.py
import json
from app.logging_config import logger
from app.config import settings

def emit_metric(metric_name: str, value: float, unit: str = "None", **dimensions):
    """Emit CloudWatch metric using EMF."""
    metric_data = {
        "_aws": {
            "Timestamp": int(time.time() * 1000),
            "CloudWatchMetrics": [
                {
                    "Namespace": "CatHouse/Services",
                    "Dimensions": [["Service"]],
                    "Metrics": [
                        {
                            "Name": metric_name,
                            "Unit": unit
                        }
                    ]
                }
            ]
        },
        "Service": settings.SERVICE_NAME,
        metric_name: value,
        **dimensions
    }
    
    # Print as JSON to stdout (CloudWatch will parse it)
    print(json.dumps(metric_data))

# Usage in middleware
async def metrics_middleware(request: Request, call_next):
    start_time = time.time()
    response = await call_next(request)
    duration = (time.time() - start_time) * 1000  # ms
    
    # Emit CloudWatch metric
    emit_metric(
        "RequestLatency",
        duration,
        unit="Milliseconds",
        Method=request.method,
        Endpoint=request.url.path
    )
    
    return response
```

### CloudWatch Dashboard Configuration

**Service Health Dashboard:**

```json
{
  "widgets": [
    {
      "type": "metric",
      "properties": {
        "title": "Service Health Status",
        "metrics": [
          ["CatHouse/Services", "service_health_status", {"service": "auth-service", "stat": "Average"}],
          [".", ".", {"service": "catalog-service"}],
          [".", ".", {"service": "installation-service"}],
          [".", ".", {"service": "proxy-service"}],
          [".", ".", {"service": "health-aggregator"}]
        ],
        "period": 60,
        "region": "us-east-1",
        "yAxis": {"left": {"min": 0, "max": 1}}
      }
    },
    {
      "type": "metric",
      "properties": {
        "title": "ECS Task Count",
        "metrics": [
          ["AWS/ECS", "RunningTaskCount", {"ServiceName": "cat-house-staging-auth-service"}],
          [".", ".", {"ServiceName": "cat-house-staging-catalog-service"}],
          [".", ".", {"ServiceName": "cat-house-staging-installation-service"}],
          [".", ".", {"ServiceName": "cat-house-staging-proxy-service"}],
          [".", ".", {"ServiceName": "cat-house-staging-health-aggregator"}]
        ],
        "period": 300,
        "region": "us-east-1"
      }
    },
    {
      "type": "metric",
      "properties": {
        "title": "HTTP Request Rate",
        "metrics": [
          ["CatHouse/Services", "http_requests_total", {"stat": "Sum", "period": 60}]
        ],
        "period": 300,
        "region": "us-east-1"
      }
    },
    {
      "type": "metric",
      "properties": {
        "title": "Error Rate by Service",
        "metrics": [
          ["CatHouse/Services", "ErrorCount", {"service": "auth-service"}],
          [".", ".", {"service": "catalog-service"}],
          [".", ".", {"service": "installation-service"}],
          [".", ".", {"service": "proxy-service"}],
          [".", ".", {"service": "health-aggregator"}]
        ],
        "period": 300,
        "stat": "Sum"
      }
    },
    {
      "type": "log",
      "properties": {
        "title": "Recent Errors",
        "query": "SOURCE '/ecs/cat-house/staging/auth-service' '/ecs/cat-house/staging/catalog-service' '/ecs/cat-house/staging/installation-service' '/ecs/cat-house/staging/proxy-service' '/ecs/cat-house/staging/health-aggregator' | fields @timestamp, service, message, trace_id | filter level = 'ERROR' | sort @timestamp desc | limit 20",
        "region": "us-east-1"
      }
    }
  ]
}
```

**API Performance Dashboard:**

```json
{
  "widgets": [
    {
      "type": "metric",
      "properties": {
        "title": "Request Latency (p50, p95, p99)",
        "metrics": [
          ["CatHouse/Services", "http_request_duration_seconds", {"stat": "p50"}],
          [".", ".", {"stat": "p95"}],
          [".", ".", {"stat": "p99"}]
        ],
        "period": 300,
        "region": "us-east-1",
        "yAxis": {"left": {"label": "Seconds"}}
      }
    },
    {
      "type": "metric",
      "properties": {
        "title": "Requests In Progress",
        "metrics": [
          ["CatHouse/Services", "http_requests_in_progress", {"stat": "Average"}]
        ],
        "period": 60,
        "region": "us-east-1"
      }
    },
    {
      "type": "metric",
      "properties": {
        "title": "HTTP Status Codes",
        "metrics": [
          ["CatHouse/Services", "http_requests_total", {"status": "2xx", "stat": "Sum"}],
          [".", ".", {"status": "4xx"}],
          [".", ".", {"status": "5xx"}]
        ],
        "period": 300,
        "region": "us-east-1"
      }
    },
    {
      "type": "metric",
      "properties": {
        "title": "Database Query Duration",
        "metrics": [
          ["CatHouse/Services", "db_query_duration_seconds", {"stat": "Average"}],
          [".", ".", {"stat": "p95"}]
        ],
        "period": 300,
        "region": "us-east-1"
      }
    },
    {
      "type": "metric",
      "properties": {
        "title": "Active DB Connections",
        "metrics": [
          ["CatHouse/Services", "db_connections_active", {"service": "auth-service"}],
          [".", ".", {"service": "catalog-service"}],
          [".", ".", {"service": "installation-service"}],
          [".", ".", {"service": "proxy-service"}]
        ],
        "period": 60,
        "stat": "Average"
      }
    }
  ]
}
```

**Business Metrics Dashboard:**

```json
{
  "widgets": [
    {
      "type": "metric",
      "properties": {
        "title": "User Registrations (Hourly)",
        "metrics": [
          ["CatHouse/Services", "business_events_total", {"event_type": "user_created", "stat": "Sum"}]
        ],
        "period": 3600,
        "region": "us-east-1"
      }
    },
    {
      "type": "metric",
      "properties": {
        "title": "Cat Installations (Daily)",
        "metrics": [
          ["CatHouse/Services", "business_events_total", {"event_type": "installation_created", "stat": "Sum"}]
        ],
        "period": 86400,
        "region": "us-east-1"
      }
    },
    {
      "type": "metric",
      "properties": {
        "title": "Active Users (DAU/MAU)",
        "metrics": [
          ["CatHouse/Services", "business_events_total", {"event_type": "user_login", "stat": "Sum"}]
        ],
        "period": 86400,
        "region": "us-east-1"
      }
    }
  ]
}
```

**Save Dashboards to Version Control:**

```bash
# cat-house-backend/terraform/cloudwatch_dashboards/
├── service_health.json
├── api_performance.json
├── database_metrics.json
└── business_metrics.json
```

**Deploy Dashboards with Terraform:**

```hcl
# cat-house-backend/terraform/cloudwatch_dashboards.tf
resource "aws_cloudwatch_dashboard" "service_health" {
  dashboard_name = "cat-house-${var.environment}-service-health"
  dashboard_body = file("${path.module}/cloudwatch_dashboards/service_health.json")
}

resource "aws_cloudwatch_dashboard" "api_performance" {
  dashboard_name = "cat-house-${var.environment}-api-performance"
  dashboard_body = file("${path.module}/cloudwatch_dashboards/api_performance.json")
}

resource "aws_cloudwatch_dashboard" "business_metrics" {
  dashboard_name = "cat-house-${var.environment}-business-metrics"
  dashboard_body = file("${path.module}/cloudwatch_dashboards/business_metrics.json")
}
```

### CloudWatch Alarms Configuration

**Create SNS Topics:**

```hcl
# cat-house-backend/terraform/sns.tf
resource "aws_sns_topic" "critical_alerts" {
  name = "cat-house-${var.environment}-critical-alerts"

  tags = {
    Environment = var.environment
    AlertLevel  = "critical"
  }
}

resource "aws_sns_topic" "warning_alerts" {
  name = "cat-house-${var.environment}-warning-alerts"

  tags = {
    Environment = var.environment
    AlertLevel  = "warning"
  }
}

resource "aws_sns_topic_subscription" "critical_email" {
  topic_arn = aws_sns_topic.critical_alerts.arn
  protocol  = "email"
  endpoint  = var.alert_email
}

resource "aws_sns_topic_subscription" "warning_email" {
  topic_arn = aws_sns_topic.warning_alerts.arn
  protocol  = "email"
  endpoint  = var.alert_email
}

# Optional: Slack webhook
resource "aws_sns_topic_subscription" "critical_slack" {
  count     = var.slack_webhook_url != "" ? 1 : 0
  topic_arn = aws_sns_topic.critical_alerts.arn
  protocol  = "https"
  endpoint  = var.slack_webhook_url
}
```

**Critical Alarm: Service Down**

```hcl
# cat-house-backend/terraform/alarms.tf
locals {
  services = [
    "auth-service",
    "catalog-service",
    "installation-service",
    "proxy-service",
    "health-aggregator"
  ]
}

resource "aws_cloudwatch_metric_alarm" "service_down" {
  for_each = toset(local.services)

  alarm_name          = "${var.environment}-${each.key}-down"
  comparison_operator = "LessThanThreshold"
  evaluation_periods  = 2
  metric_name         = "RunningTaskCount"
  namespace           = "AWS/ECS"
  period              = 60
  statistic           = "Average"
  threshold           = 1
  alarm_description   = "${each.key} has no running tasks"
  alarm_actions       = [aws_sns_topic.critical_alerts.arn]
  treat_missing_data  = "breaching"

  dimensions = {
    ServiceName = "cat-house-${var.environment}-${each.key}"
    ClusterName = "cat-house-${var.environment}"
  }

  tags = {
    Service     = each.key
    Environment = var.environment
    Severity    = "critical"
    Runbook     = "https://github.com/yourorg/cat-house/wiki/Runbook-Service-Down"
  }
}
```

**Critical Alarm: High Error Rate**

```hcl
resource "aws_cloudwatch_metric_alarm" "high_error_rate" {
  for_each = toset(local.services)

  alarm_name          = "${var.environment}-${each.key}-high-error-rate"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 2
  threshold           = 10  # More than 10 errors in 5 minutes
  alarm_description   = "${each.key} has high error rate"
  alarm_actions       = [aws_sns_topic.critical_alerts.arn]
  treat_missing_data  = "notBreaching"

  metric_query {
    id          = "error_rate"
    expression  = "m1 / m2 * 100"
    label       = "Error Rate %"
    return_data = true
  }

  metric_query {
    id = "m1"
    metric {
      metric_name = "ErrorCount"
      namespace   = "CatHouse/Services"
      period      = 300
      stat        = "Sum"
      dimensions = {
        service = each.key
      }
    }
  }

  metric_query {
    id = "m2"
    metric {
      metric_name = "http_requests_total"
      namespace   = "CatHouse/Services"
      period      = 300
      stat        = "Sum"
      dimensions = {
        service = each.key
      }
    }
  }

  tags = {
    Service     = each.key
    Environment = var.environment
    Severity    = "critical"
    Runbook     = "https://github.com/yourorg/cat-house/wiki/Runbook-High-Error-Rate"
  }
}
```

**Warning Alarm: High Latency**

```hcl
resource "aws_cloudwatch_metric_alarm" "high_latency" {
  for_each = toset(local.services)

  alarm_name          = "${var.environment}-${each.key}-high-latency"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 3
  datapoints_to_alarm = 2
  threshold           = 0.5  # 500ms
  alarm_description   = "${each.key} p95 latency > 500ms"
  alarm_actions       = [aws_sns_topic.warning_alerts.arn]
  treat_missing_data  = "notBreaching"

  metric_name = "http_request_duration_seconds"
  namespace   = "CatHouse/Services"
  period      = 300
  statistic   = "p95"

  dimensions = {
    service = each.key
  }

  tags = {
    Service     = each.key
    Environment = var.environment
    Severity    = "warning"
    Runbook     = "https://github.com/yourorg/cat-house/wiki/Runbook-High-Latency"
  }
}
```

**Warning Alarm: High CPU/Memory**

```hcl
resource "aws_cloudwatch_metric_alarm" "high_cpu" {
  for_each = toset(local.services)

  alarm_name          = "${var.environment}-${each.key}-high-cpu"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 3
  metric_name         = "CPUUtilization"
  namespace           = "AWS/ECS"
  period              = 300
  statistic           = "Average"
  threshold           = 80
  alarm_description   = "${each.key} CPU > 80%"
  alarm_actions       = [aws_sns_topic.warning_alerts.arn]

  dimensions = {
    ServiceName = "cat-house-${var.environment}-${each.key}"
    ClusterName = "cat-house-${var.environment}"
  }

  tags = {
    Service     = each.key
    Environment = var.environment
    Severity    = "warning"
    Runbook     = "https://github.com/yourorg/cat-house/wiki/Runbook-High-CPU"
  }
}

resource "aws_cloudwatch_metric_alarm" "high_memory" {
  for_each = toset(local.services)

  alarm_name          = "${var.environment}-${each.key}-high-memory"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 3
  metric_name         = "MemoryUtilization"
  namespace           = "AWS/ECS"
  period              = 300
  statistic           = "Average"
  threshold           = 85
  alarm_description   = "${each.key} memory > 85%"
  alarm_actions       = [aws_sns_topic.warning_alerts.arn]

  dimensions = {
    ServiceName = "cat-house-${var.environment}-${each.key}"
    ClusterName = "cat-house-${var.environment}"
  }

  tags = {
    Service     = each.key
    Environment = var.environment
    Severity    = "warning"
    Runbook     = "https://github.com/yourorg/cat-house/wiki/Runbook-High-Memory"
  }
}
```

### CloudWatch Logs Insights Queries

**Save these queries in CloudWatch Logs Insights for quick access:**

**1. Find All Errors in Last Hour:**
```
fields @timestamp, service, message, trace_id, function, line
| filter level = "ERROR"
| sort @timestamp desc
| limit 100
```

**2. Track Request by Trace ID:**
```
fields @timestamp, service, level, message, extra
| filter trace_id = "YOUR_TRACE_ID_HERE"
| sort @timestamp asc
```

**3. Calculate Average Response Time by Endpoint:**
```
fields @timestamp, extra.path as endpoint, extra.duration_ms as duration
| filter extra.duration_ms > 0
| stats avg(duration) as avg_duration, 
        max(duration) as max_duration, 
        count(*) as request_count 
  by endpoint
| sort avg_duration desc
```

**4. Error Rate by Service:**
```
fields @timestamp, service
| stats count(*) as total, 
        sum(level = "ERROR") as errors 
  by service
| fields service, errors, total, (errors / total * 100) as error_rate
| sort error_rate desc
```

**5. Slow Queries (> 200ms):**
```
fields @timestamp, service, extra.path, extra.duration_ms, trace_id
| filter extra.duration_ms > 200
| sort extra.duration_ms desc
| limit 50
```

**6. Requests per Minute:**
```
fields @timestamp
| filter extra.method != ""
| stats count(*) as requests by bin(5m)
```

**7. User Activity by Trace:**
```
fields @timestamp, trace_id, extra.path, extra.method, extra.status_code
| filter trace_id like /YOUR_PREFIX/
| sort @timestamp asc
```

**8. Database Connection Issues:**
```
fields @timestamp, service, message
| filter message like /database|connection|pool/
| sort @timestamp desc
| limit 100
```

### Alarm Runbooks

**Runbook Template:**

```markdown
# Runbook: [Alarm Name]

## Severity
[Critical / Warning / Info]

## Description
[What the alarm indicates]

## Impact
[User impact and business consequences]

## Diagnosis Steps

1. **Check CloudWatch Dashboard**
   - Open: [Dashboard URL]
   - Look for: [Specific metrics]

2. **Check Service Logs**
   ```bash
   # View recent logs
   aws logs tail /ecs/cat-house/[environment]/[service] --follow
   
   # Search for errors
   aws logs tail /ecs/cat-house/[environment]/[service] \
     --filter-pattern "ERROR" --since 1h
   ```

3. **Check Service Health**
   ```bash
   # Check ECS task status
   aws ecs describe-services \
     --cluster cat-house-[environment] \
     --services cat-house-[environment]-[service]
   
   # Check task count
   aws ecs list-tasks \
     --cluster cat-house-[environment] \
     --service-name cat-house-[environment]-[service]
   ```

4. **Check Recent Deployments**
   ```bash
   # Check recent task definition changes
   aws ecs describe-task-definition \
     --task-definition cat-house-[environment]-[service]
   ```

## Resolution Steps

### If [Condition A]
1. [Step 1]
2. [Step 2]
3. [Verification]

### If [Condition B]
1. [Step 1]
2. [Step 2]
3. [Verification]

## Rollback Procedure

```bash
# Rollback to previous task definition
aws ecs update-service \
  --cluster cat-house-[environment] \
  --service cat-house-[environment]-[service] \
  --task-definition cat-house-[environment]-[service]:[PREVIOUS_REVISION]

# Force new deployment
aws ecs update-service \
  --cluster cat-house-[environment] \
  --service cat-house-[environment]-[service] \
  --force-new-deployment
```

## Escalation

- If not resolved in 15 minutes: Contact on-call engineer
- If critical and not resolved in 30 minutes: Page engineering manager
- Emergency contact: [Contact info]

## Post-Incident

1. Document what happened
2. Create GitHub issue for root cause analysis
3. Update runbook if needed
4. Schedule post-mortem if critical
```

**Example: Service Down Runbook**

```markdown
# Runbook: Service Down

## Severity
Critical

## Description
ECS service has 0 running tasks. The service is completely unavailable.

## Impact
- API endpoints return 503 errors
- Users cannot access affected functionality
- Data loss possible if prolonged

## Diagnosis Steps

1. Check ECS console for stopped tasks
2. View task stopped reason:
   ```bash
   aws ecs describe-tasks \
     --cluster cat-house-staging \
     --tasks [TASK_ID] \
     --query 'tasks[0].stoppedReason'
   ```

3. Check CloudWatch Logs for errors before crash

4. Check if database is accessible:
   ```bash
   # From local machine with Neon connection string
   psql $DATABASE_URL -c "SELECT 1;"
   ```

## Resolution Steps

### If Task Failed Health Check
1. Check health endpoint manually
2. Review application logs for startup errors
3. Verify database migrations completed
4. Redeploy if needed

### If Database Connection Failed
1. Verify Neon database is accessible
2. Check GitHub Secrets has correct DATABASE_URL configured
3. Verify environment variables are passed to ECS task definition
4. Check network connectivity from ECS to Neon

### If Out of Memory
1. Increase memory in task definition
2. Check for memory leaks in logs
3. Redeploy with updated task definition

## Escalation
Page on-call immediately for production. Resolve within 15 minutes or escalate to engineering manager.
```

### Testing Checklist

- [ ] **Metrics Collection**
  - [ ] Verify /metrics endpoint returns data
  - [ ] Check Prometheus metrics appear in CloudWatch
  - [ ] Test custom business metrics tracking
  - [ ] Verify metrics from all 5 services

- [ ] **Dashboards**
  - [ ] Deploy all dashboards via Terraform
  - [ ] Verify widgets show data
  - [ ] Test dashboard refresh
  - [ ] Share dashboard URLs with team

- [ ] **Alarms**
  - [ ] Verify all alarms created
  - [ ] Test alarm by triggering threshold
  - [ ] Verify SNS notifications received
  - [ ] Test alarm clear notification
  - [ ] Verify Slack integration (if configured)

- [ ] **Log Queries**
  - [ ] Save all queries in CloudWatch
  - [ ] Test each query returns expected data
  - [ ] Share query links with team

- [ ] **Runbooks**
  - [ ] Write runbook for each alarm
  - [ ] Link runbooks in alarm tags
  - [ ] Test runbook procedures
  - [ ] Train team on runbooks

### Distributed Tracing (Phase 2 - Deferred)

OpenTelemetry and AWS X-Ray integration deferred to post-MVP. Current correlation IDs provide basic request tracing across services.

**Future Implementation:**
- Install opentelemetry-api and opentelemetry-sdk
- Configure OTLP exporter for AWS X-Ray
- Instrument FastAPI with FastAPIInstrumentor
- Instrument httpx for cross-service tracing
- Enable X-Ray daemon in ECS tasks

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-30 | 1.0 | Initial story creation | Sarah |

---

## Dev Agent Record

### Agent Model Used
_To be populated during implementation_

### Debug Log References
_To be populated during implementation_

### Completion Notes List
_To be populated during implementation_

### File List
_To be populated during implementation_

---

## QA Results
_To be populated by QA Agent_
